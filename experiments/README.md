# A Matter of Time: Revealing the Structure of Time in Vision-Language Models

[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Paper](https://img.shields.io/badge/paper-arXiv-red.svg)](https://arxiv.org/abs/YOUR_ARXIV_ID)
[![Dataset](https://img.shields.io/badge/dataset-TIME10k-orange.svg)](https://osf.io/4th79/?view_only=560f540a7bac4d489faf164b16109642)

![Teaser](ressources/teaser.png)

This repository contains the official implementation of **"A Matter of Time: Revealing the Structure of Time in Vision-Language Models"**.

## ğŸ“„ Abstract

Large-scale vision-language models (VLMs) such as CLIP have gained popularity for their generalizable and expressive multimodal representations. This paper investigates the temporal awareness of VLMs, assessing their ability to position visual content in time. We introduce TIME10k, a benchmark dataset of over 10,000 images with temporal ground truth, and evaluate the time-awareness of 37 VLMs. Our investigation reveals that temporal information is structured along a low-dimensional, non-linear manifold in the VLM embedding space. Based on this insight, we propose methods to derive explicit "timeline" representations from the embedding space.

## ğŸš€ Key Features

- **TIME10k Dataset**: 10,000+ temporally annotated images across 6 object categories
- **Comprehensive Evaluation**: Time-awareness assessment of 37 state-of-the-art VLMs
- **Novel Timeline Modeling**: UMAP and BÃ©zier curve-based approaches for temporal representation
- **Efficient Inference**: Timeline methods achieve competitive accuracy while being computationally efficient

## ğŸ“Š Performance Overview

![Time probing](ressources/performance_scatter.png)

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- CUDA 11.0+ (for GPU support)
- 32GB+ RAM recommended

### Setup

1. Clone the repository:
```bash
git clone https://github.com/YOUR_USERNAME/time-vlm.git
cd time-vlm
```

2. Create a conda environment:
```bash
conda create -n time-vlm python=3.8
conda activate time-vlm
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Install model-specific dependencies:
```bash
bash install_models.sh
```

## ğŸ“ Project Structure

```
time-vlm/
â”œâ”€â”€ configs/              # Configuration files
â”œâ”€â”€ data/                # Dataset utilities
â”œâ”€â”€ models/              # Model implementations
â”‚   â”œâ”€â”€ clip/           # CLIP variants
â”‚   â”œâ”€â”€ eva_clip/       # EVA-CLIP models
â”‚   â”œâ”€â”€ imagebind/      # ImageBind
â”‚   â”œâ”€â”€ openclip/       # OpenCLIP models
â”‚   â””â”€â”€ vit_lens/       # ViT-Lens
â”œâ”€â”€ evaluation/          # Evaluation scripts
â”‚   â”œâ”€â”€ time_probing.py
â”‚   â”œâ”€â”€ timeline_umap.py
â”‚   â””â”€â”€ timeline_bezier.py
â”œâ”€â”€ encodings/          # Pre-computed embeddings
â”œâ”€â”€ resources/          # Images and figures
â”œâ”€â”€ utils/              # Utility functions
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ install_models.sh
â””â”€â”€ README.md
```

## ğŸƒ Quick Start

### 1. Time Probing Evaluation

Evaluate temporal awareness using prompt-based approach:

```python
python evaluation/time_probing.py \
    --model clip-vit-b32 \
    --data_path /path/to/time10k \
    --output_dir results/
```

### 2. Timeline Modeling

#### UMAP-based Timeline:
```python
python evaluation/timeline_umap.py \
    --model clip-vit-b32 \
    --embeddings_path encodings/ \
    --optimize_params
```

#### BÃ©zier Curve Timeline:
```python
python evaluation/timeline_bezier.py \
    --model eva-clip-l14 \
    --embeddings_path encodings/eva/ \
    --num_control_points 200
```

### 3. Evaluate Multiple Models

```bash
python run_experiments.py --config configs/full_evaluation.yaml
```

## ğŸ“ˆ Results

### Time Probing Performance

| Model | MAE â†“ | TAI â†‘ |
|-------|-------|-------|
| ... | ... | ... |

*Full results table to be added*

### Timeline Modeling Results

Our timeline approaches achieve competitive accuracy compared to time probing while being significantly more efficient:

- **UMAP Timeline**: ~50x faster inference
- **BÃ©zier Timeline**: ~100x faster inference

## ğŸ—‚ï¸ TIME10k Dataset

The TIME10k dataset contains 10,091 images across 6 categories:
- Cars (4,393)
- Mobile Phones (4,337)
- Ships (841)
- Musical Instruments (436)
- Aircraft (69)
- Weapons & Ammunition (15)

Access the dataset: [TIME10k on OSF](https://osf.io/4th79/?view_only=560f540a7bac4d489faf164b16109642)

## ğŸ“ Citation

If you find this work useful, please cite:

```bibtex
@inproceedings{author2024time,
  title={A Matter of Time: Revealing the Structure of Time in Vision-Language Models},
  author={Anonymous},
  booktitle={Conference},
  year={2024}
}
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

todo
## ğŸ™ Acknowledgments

todo: cultural heritage program? eva clip and the others? 

---

For questions or issues, please open an issue on GitHub or contact [nidham.tekaya@fhstp.ac.at].